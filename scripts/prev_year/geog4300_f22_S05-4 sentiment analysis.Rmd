---
title: "Sentiment analysis"
output: html_notebook
editor_options: 
  chunk_output_type: console
---

```{r}
library(gutenbergr)
library(tidytext)
library(tidyverse)

douglass<-gutenberg_download(23) 

douglass_words<- douglass  %>%
  unnest_tokens(word,text)
```

We can use tidytext to do sentiment analysis. It includes a basic list of words and associated emotions:

```{r}
sentiments
```

We can see the list of sentiments by using table.

```{r}
table(sentiments$sentiment)
```

There's actually *three* different sentiment dictionaries: bing, AFIN and nrc. 

```{r}
bing_sentiment<-get_sentiments("bing")
head(bing_sentiment)
```


What if we want to look for positive and negative words? Here, we just use the Bing sentiment dictionary.

```{r}
douglass_posneg<-douglass_words %>%
  inner_join(bing_sentiment) %>% #This keeps only words in both lists
  anti_join(stop_words) %>%
  group_by(word,sentiment) %>%
  summarise(n=n())
```

You can then visualize the ten most common words in each sentiment. Note how the reorder function is used here to arrange words from highest to lowest count using the n variable.

```{r}
douglass_top<-douglass_posneg %>%
  group_by(sentiment) %>%
  top_n(10) 

ggplot(douglass_top,aes(reorder(word,n), n, fill = sentiment)) + 
  geom_col()+
  facet_wrap(~sentiment,scales="free_y")+
  coord_flip()
```

We can also look at sentiment by sentence. Here, we group by sentences and label those as "lines". The second function then separates sentences into individual words but keeps the line variable

```{r}
douglass_sentences<-douglass %>%
  unnest_tokens(sentence,text,token="sentences") %>%
  mutate(line=row_number()) 

douglass_sentences_long<-douglass_sentences %>%
  unnest_tokens(word,sentence)
```

We can then count the number of positive and negative words by sentence. We use the spread function to create separate columns for positive and negative and use the fill parameter to add 0's where there would be NAs.

```{r}
douglass_sentences_posneg<-douglass_sentences_long %>%
  inner_join(bing_sentiment) %>%
  anti_join(stop_words) %>%
  group_by(line,sentiment) %>%
  summarise(count=n()) %>%
  pivot_wider(names_from=sentiment,
              values_from=count,
              values_fill=0)
head(douglass_sentences_posneg)
```

Let's create a simple metric, "score", that counts the net positive or negative words in a sentence.

```{r}
douglass_sentences_posneg <-douglass_sentences_posneg %>%
  mutate(score=positive-negative)
head(douglass_sentences_posneg)
```

Let's visualize that. Here's the overall distribution of sentiment scores:

```{r}
ggplot(douglass_sentences_posneg,aes(x=score)) +
  geom_histogram(binwidth=0.5)
```

Here's a graph looking at the scores throughout the length of the book: 

```{r}
ggplot(douglass_sentences_posneg,aes(x=line,y=score))+
  geom_bar(stat="identity")
```

What's the most negative sentence? The most positive one?

```{r}
#Find line numbers
douglass_min<-douglass_sentences_posneg %>%
  filter(score==min(douglass_sentences_posneg$score))

douglass_max<-douglass_sentences_posneg %>%
  filter(score==max(douglass_sentences_posneg$score))

#Select min score (most negative)
douglass_minsent<-douglass_sentences %>%
  filter(line==douglass_min$line) 

as.character(douglass_minsent$sentence)

#Select max score (most positive)
douglass_maxsent<-douglass_sentences %>%
  filter(line==douglass_max[1,]$line) 

as.character(douglass_maxsent$sentence)
```

To see Julia Silge show off more about this package, visit: https://www.youtube.com/watch?v=evTuL-RcRpc
