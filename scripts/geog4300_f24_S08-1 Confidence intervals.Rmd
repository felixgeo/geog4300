---
title: "Geog6300: Confidence intervals"
output: html_notebook
editor_options: 
  chunk_output_type: console
---

```{r setup}
library(tidyverse)
```

Confidence intervals can be easily computed in R. You need to know three things: sample mean, standard error, and z-score. Suppose you have a sample (n=174) with a mean of 43 and a standard deviation of 6.2. For a 95% confidence interval, do the following:

```{r}
mean<-43
se<-(6.2/sqrt(174))
z.score<-qnorm(0.975) #This function gets the z score associated with a .975 probability
error<-(se*z.score)
CI.lower<-mean-error
CI.upper<-mean+error
```

**You try it!** Adjust the code above for a 90% confidence interval and a sample size of only 80.

```{r}

```


We can also visualize these confidence intervals. Let's load the microdata you are working within the current lab.

```{r}
cps_data<-read_csv("data/IPUMS_CPS_FoodSec.csv")

#Or if you need the data from Github:
#cps_data<-read_csv("https://github.com/jshannon75/geog4300/raw/master/data/IPUMS_CPS_FoodSec.csv")

```

What if we're looking for confidence intervals for mean earnings per week (the EARNWEEK variable) for in the South? First, we need to filter for only responses in the South and ones with a meaningful response to the EARNWEEK variable. See the codebook for more information--9999.99 is the NIU code.

**Note: because income is usually skewed, we usually use median rather than mean. But mean is used here for illustration purposes.

```{r}
cps_data_south<-cps_data %>%
  filter(Region=="South Region" & EARNWEEK<9000)
```

Now let's calculate the mean and standard deviation by state. We can also calculate the standard error, which is the sd divided by the square root of the number of responses.

```{r}
cps_data_south_summary<-cps_data_south %>%
  group_by(STATE) %>%
  summarise(earn_mean=mean(EARNWEEK),
            earn_sd=sd(EARNWEEK),
            responses=n(),
            se=earn_sd/sqrt(responses))
```

We also need the z score for 95% confidence. This this includes both the top and bottom end, we want 2.5% on either side of the distribution (0.025 and 0.975 as the cutoffs). We'll use .975 in this case to get a positive number, but notice that the z.score for 0.025 is the same--just negative.

```{r}
z.score_upper=qnorm(0.975)
z.score_lower=qnorm(0.025)
```


Lastly, we can calculate the error term, which is the z score for a 95% confidence interval times the standard error, and the resulting confidence interval. We will use the z score we already calculated above.

```{r}
cps_data_south_summary<-cps_data_south_summary %>%
  mutate(error=z.score_upper*se,
         ci_high=earn_mean+error,
         ci_low=earn_mean-error)
```

We've got our confidence interval! We can visualize these by state using geom_point for the means and geom_linerange (a new geom for us) for the confidence intervals. 

```{r}
ggplot(cps_data_south_summary %>% mutate(STATE=reorder(STATE,earn_mean)))+
  geom_linerange(aes(xmin=ci_low,xmax=ci_high,y=STATE,group=STATE)) +
  geom_point(aes(x=earn_mean,y=STATE))
```

Note that in the ggplot function we used reorder to sort states by the earn_mean variable. 

**You try it!** Calculate the confidence intervals for weekly income by census region (the Region variable). If you get that done, try to style your axes to make them more meaningful, changing the labels and showing the numeric labels as dollars. Here's one forum thread that tries to tackle that latter problem: https://forum.posit.co/t/adding-currency-dollar-sign-to-each-bar-in-a-bar-graph/142128/2

```{r}

```


###CI for proportions. 

Confidence intervals for proportions would require a different standard error calculation. If 64.75% of geographers (n=400) voted to hold the next AAG in Miami, here's how we'd figure that out. 

```{r}
p<-0.6475
se<-sqrt((.6475*(1-.6475))/399)
z.score<-qnorm(.975)
moe<-se*z.score
CI.lower<-p-moe
CI.upper<-p+moe
```

Let's look at that in the CPS dataset for the Hispanic variable. This code gives us a count of responses by state as well as the total number of responses. Again, check the codebook for more information.

```{r}
cps_data_hisp<-cps_data %>%
  filter(HISPAN<900) %>% #Filters NIU
  group_by(STATE) %>%
  mutate(totpop=n()) %>% #Mutate adds the total responses but keeps indivdiual obervations
  ungroup() %>%
  group_by(STATE,HISPAN,totpop) %>%
  summarise(count=n()) %>%
  pivot_wider(names_from=HISPAN,values_from=count)
```

To calculate the percent identifying as Mexican, we can do the following.

```{r}
cps_data_hisp<-cps_data_hisp %>%
  mutate(mex_pct=`100`/totpop)
```

We can then calculate the standard error and create a CI.

```{r}
cps_data_hisp<-cps_data_hisp %>%
  mutate(se=sqrt((mex_pct*(1-mex_pct))/(totpop-1)),
         error=z.score*se,
         ci_high=mex_pct+error,
         ci_low=mex_pct-error)
```

Lastly, let's visualize those numbers.

```{r}
ggplot(cps_data_hisp) +
  geom_linerange(aes(xmin=ci_low,xmax=ci_high,y=reorder(STATE,mex_pct),group=STATE))+
  geom_point(aes(x=mex_pct,y=STATE))
```

**You try it!** Calculate the percent of the population who have a marital status of divorced by each state. It's the `MARST` variable, and divorced is code 4. Also calculate the confidence interval by each state. Then graph it just like the percent who are Mexican in the code above.

```{r}

```

